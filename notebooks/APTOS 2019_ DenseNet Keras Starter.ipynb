{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, we will explore the complete workflow for the APTOS 2019 competition. We will go through:\n",
    "\n",
    "1. Loading & Exploration: A quick overview of the dataset\n",
    "2. Resize Images: We will resize both the training and test images to 224x224, so that it matches the ImageNet format.\n",
    "3. Mixup & Data Generator: We show how to create a data generator that will perform random transformation to our datasets (flip vertically/horizontally, rotation, zooming). This will help our model generalize better to the data, since it is fairly small (only ~3000 images).\n",
    "4. Quadratic Weighted Kappa: A thorough overview of the metric used for this competition, with an intuitive example. Check it out!\n",
    "5. Model: We will use a DenseNet-121 pre-trained on ImageNet. We will finetune it using Adam for 15 epochs, and evaluate it on an unseen validation set.\n",
    "6. Training & Evaluation: We take a look at the change in loss and QWK score through the epochs.\n",
    "\n",
    "### Citations & Resources\n",
    "\n",
    "* I had the idea of using mixup from [KeepLearning's ResNet50 baseline](https://www.kaggle.com/mathormad/aptos-resnet50-baseline). Since the implementation was in PyTorch, I instead used an [open-sourced keras implementation](https://github.com/yu4u/mixup-generator).\n",
    "* The transfer learning procedure is mostly inspired from my [previous kernel for iWildCam](https://www.kaggle.com/xhlulu/densenet-transfer-learning-iwildcam-2019). The workflow was however heavily modified since then.\n",
    "* Used similar [method as Abhishek](https://www.kaggle.com/abhishek/optimizer-for-quadratic-weighted-kappa) to find the optimal threshold.\n",
    "* [Lex's kernel](https://www.kaggle.com/lextoumbourou/blindness-detection-resnet34-ordinal-targets) prompted me to try using Multilabel instead of multiclass classification, which slightly improved the kappa score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.604950Z",
     "start_time": "2019-07-15T20:18:17.057181Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.647857Z",
     "start_time": "2019-07-15T20:18:18.606263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3662, 2)\n",
      "(1928, 1)\n",
      "(3662, 8) (1928, 7)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "train_meta = pd.read_csv('../data/train_meta.csv')\n",
    "test_meta = pd.read_csv('../data/test_meta.csv')\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(train_meta.shape, test_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.657283Z",
     "start_time": "2019-07-15T20:18:18.648976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>image_shape</th>\n",
       "      <th>image_size</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>width_height_ratio</th>\n",
       "      <th>width_height_added</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "      <td>(2136, 3216, 3)</td>\n",
       "      <td>3218676</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>5352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "      <td>(2136, 3216, 3)</td>\n",
       "      <td>2261129</td>\n",
       "      <td>2136</td>\n",
       "      <td>3216</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>5352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "      <td>(1736, 2416, 3)</td>\n",
       "      <td>1882172</td>\n",
       "      <td>1736</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.718543</td>\n",
       "      <td>4152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "      <td>(1050, 1050, 3)</td>\n",
       "      <td>975218</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "      <td>(1536, 2048, 3)</td>\n",
       "      <td>1819430</td>\n",
       "      <td>1536</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis      image_shape  image_size  height  width  \\\n",
       "0  000c1434d8d7          2  (2136, 3216, 3)     3218676    2136   3216   \n",
       "1  001639a390f0          4  (2136, 3216, 3)     2261129    2136   3216   \n",
       "2  0024cdab0c1e          1  (1736, 2416, 3)     1882172    1736   2416   \n",
       "3  002c21358ce6          0  (1050, 1050, 3)      975218    1050   1050   \n",
       "4  005b95c28852          0  (1536, 2048, 3)     1819430    1536   2048   \n",
       "\n",
       "   width_height_ratio  width_height_added  \n",
       "0            0.664179                5352  \n",
       "1            0.664179                5352  \n",
       "2            0.718543                4152  \n",
       "3            1.000000                2100  \n",
       "4            0.750000                3584  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.669602Z",
     "start_time": "2019-07-15T20:18:18.658486Z"
    }
   },
   "outputs": [],
   "source": [
    "m_train = train_meta[['image_size', 'height', 'width', 'width_height_ratio', 'width_height_added']].values\n",
    "m_test = test_meta[['image_size', 'height', 'width', 'width_height_ratio', 'width_height_added']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.680925Z",
     "start_time": "2019-07-15T20:18:18.671094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS = StandardScaler()\n",
    "SS.fit(m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.703895Z",
     "start_time": "2019-07-15T20:18:18.681846Z"
    }
   },
   "outputs": [],
   "source": [
    "m_train = SS.transform(m_train)\n",
    "m_test = SS.transform(m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.708147Z",
     "start_time": "2019-07-15T20:18:18.705499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3662, 5), (1928, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train.shape, m_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.718615Z",
     "start_time": "2019-07-15T20:18:18.709722Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, desired_size=224):\n",
    "    im = Image.open(image_path)\n",
    "    im = im.resize((desired_size, )*2, resample=Image.LANCZOS)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:18.729697Z",
     "start_time": "2019-07-15T20:18:18.719938Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_ben_color(path, size, crop=False, sigmaX=10):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    if crop:\n",
    "        image = crop_image_from_gray(image)\n",
    "    image = cv2.resize(image, (size, size))\n",
    "    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "    return image\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:25.529208Z",
     "start_time": "2019-07-15T20:18:18.730635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3662/3662 [00:06<00:00, 539.69it/s]\n"
     ]
    }
   ],
   "source": [
    "size = 256\n",
    "N = train_df.shape[0]\n",
    "x_train = np.empty((N, size, size, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_id in enumerate(tqdm(train_df['id_code'])):\n",
    "    x_train[i, :, :, :] = np.load(\n",
    "        f'../data/train_images/npy_bengrahm_color/{image_id}.npy'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:44.737519Z",
     "start_time": "2019-07-15T20:18:25.531293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1928/1928 [01:19<00:00, 24.34it/s]\n"
     ]
    }
   ],
   "source": [
    "N = test_df.shape[0]\n",
    "x_test = np.empty((N, size, size, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_id in enumerate(tqdm(test_df['id_code'])):\n",
    "    x_test[i, :, :, :] = load_ben_color(\n",
    "        f'../data/test_images/{image_id}.png', size, crop=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:44.740632Z",
     "start_time": "2019-07-15T20:19:44.738892Z"
    }
   },
   "outputs": [],
   "source": [
    "#!mkdir ../data/npy_files\n",
    "#np.save('../data/npy_files/x_train.npy', x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:44.752851Z",
     "start_time": "2019-07-15T20:19:44.742102Z"
    }
   },
   "outputs": [],
   "source": [
    "#x_train = np.load('../data/npy_files/x_train.npy')\n",
    "#x_test = np.load('../data/npy_files/x_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:44.764539Z",
     "start_time": "2019-07-15T20:19:44.754332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3662, 256, 256, 3)\n",
      "(3662, 5)\n",
      "(1928, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.get_dummies(train_df['diagnosis']).values\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating multilabels\n",
    "\n",
    "Instead of predicting a single label, we will change our target to be a multilabel problem; i.e., if the target is a certain class, then it encompasses all the classes before it. E.g. encoding a class 4 retinopathy would usually be `[0, 0, 0, 1]`, but in our case we will predict `[1, 1, 1, 1]`. For more details, please check out [Lex's kernel](https://www.kaggle.com/lextoumbourou/blindness-detection-resnet34-ordinal-targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:44.775579Z",
     "start_time": "2019-07-15T20:19:44.765459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_train: [1805  370  999  193  295]\n",
      "Multilabel version: [3662 1857 1487  488  295]\n"
     ]
    }
   ],
   "source": [
    "y_train_multi = np.empty(y_train.shape, dtype=y_train.dtype)\n",
    "y_train_multi[:, 4] = y_train[:, 4]\n",
    "\n",
    "for i in range(3, -1, -1):\n",
    "    y_train_multi[:, i] = np.logical_or(y_train[:, i], y_train_multi[:, i+1])\n",
    "\n",
    "print(\"Original y_train:\", y_train.sum(axis=0))\n",
    "print(\"Multilabel version:\", y_train_multi.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:44.786886Z",
     "start_time": "2019-07-15T20:19:44.776487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3662, 256, 256, 3), (3662, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.058738Z",
     "start_time": "2019-07-15T20:19:44.787839Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train_multi, \n",
    "    test_size=0.15, \n",
    "    random_state=2019\n",
    ")\n",
    "m_train, m_val, = train_test_split(\n",
    "    m_train, \n",
    "    test_size=0.15, \n",
    "    random_state=2019\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.735059Z",
     "start_time": "2019-07-15T20:19:45.059814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.739145Z",
     "start_time": "2019-07-15T20:19:45.736164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3112, 5), (550, 5), (3112, 256, 256, 3), (550, 256, 256, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train.shape, m_val.shape, x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup & Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.751989Z",
     "start_time": "2019-07-15T20:19:45.740385Z"
    }
   },
   "outputs": [],
   "source": [
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.764287Z",
     "start_time": "2019-07-15T20:19:45.753149Z"
    }
   },
   "outputs": [],
   "source": [
    "class DualGenerator():\n",
    "    def __init__(self, X_train, m_train, y_train, batch_size=32, shuffle=True, datagen=None):\n",
    "        self.m_train = m_train\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "                m_X = self.__meta_generation(batch_ids)\n",
    "                yield [X, m_X], y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __meta_generation(self, batch_ids):\n",
    "        return self.m_train[batch_ids]\n",
    "    \n",
    "    def __data_generation(self, batch_ids):\n",
    "        X = self.X_train[batch_ids]\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "        y = self.y_train[batch_ids]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.781166Z",
     "start_time": "2019-07-15T20:19:45.765289Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def create_datagen():\n",
    "    return ImageDataGenerator(\n",
    "        zoom_range=0.15,  # set range for random zoom\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='constant',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True,  # randomly flip images\n",
    "        rescale=1.0/255.0,\n",
    "    )\n",
    "\n",
    "# Using original generator\n",
    "#data_generator = create_datagen().flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "data_generator = DualGenerator(x_train, m_train, y_train, batch_size=BATCH_SIZE, datagen=create_datagen())() \n",
    "# Using Mixup\n",
    "#mixup_generator = MixupGenerator(x_train, y_train, batch_size=BATCH_SIZE, alpha=0.2, datagen=create_datagen())()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.800504Z",
     "start_time": "2019-07-15T20:19:45.783019Z"
    }
   },
   "outputs": [],
   "source": [
    "gen = create_datagen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.956429Z",
     "start_time": "2019-07-15T20:19:45.804850Z"
    }
   },
   "outputs": [],
   "source": [
    "x = next(iter(data_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.961087Z",
     "start_time": "2019-07-15T20:19:45.957526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 256, 256, 3),\n",
       " array([[ 1.8783525 ,  0.79465274,  0.64785739, -0.31804108,  0.70756588],\n",
       "        [-0.72626487, -0.87880537, -1.09160508,  1.6082367 , -1.01625675],\n",
       "        [-1.11564773, -1.92932423, -1.55531094, -0.37002001, -1.7069134 ],\n",
       "        [-0.10726543,  0.38550329,  0.45332713, -0.61893973,  0.42989372],\n",
       "        [-0.2358391 ,  0.38550329,  0.45332713, -0.61893973,  0.42989372],\n",
       "        [ 1.56000595,  0.79465274,  0.64785739, -0.31804108,  0.70756588],\n",
       "        [-0.23749156,  0.01690018,  0.03712284, -0.37002001,  0.02959476],\n",
       "        [-1.14855411, -1.68236015, -1.35286375, -0.37243546, -1.48632612],\n",
       "        [-0.75846134, -0.87880537, -1.09160508,  1.6082367 , -1.01625675],\n",
       "        [ 1.87998625,  0.79465274,  0.64785739, -0.31804108,  0.70756588],\n",
       "        [-1.15677843, -1.92932423, -1.55531094, -0.37002001, -1.7069134 ],\n",
       "        [-0.28311265,  0.01690018,  0.03712284, -0.37002001,  0.02959476],\n",
       "        [ 0.10502746,  0.38550329,  0.45332713, -0.61893973,  0.42989372],\n",
       "        [-0.03089152,  0.38550329,  0.45332713, -0.61893973,  0.42989372],\n",
       "        [-1.17091011, -1.68236015, -1.35286375, -0.37243546, -1.48632612],\n",
       "        [-0.79345998, -0.87880537, -1.09160508,  1.6082367 , -1.01625675]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])\n",
    "x[0][0].shape, x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.972505Z",
     "start_time": "2019-07-15T20:19:45.962315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating keras callback for QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.981136Z",
     "start_time": "2019-07-15T20:19:45.973719Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_kappas = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         pdb.set_trace()\n",
    "        X_val, m_val, y_val = self.validation_data[:3]\n",
    "        y_val = y_val.sum(axis=1) - 1\n",
    "        \n",
    "        y_pred = self.model.predict([X_val, m_val]) > 0.5 # <<<<<<<<<<<<<<<<<<<\n",
    "        y_pred = y_pred.astype(int).sum(axis=1) - 1\n",
    "\n",
    "        _val_kappa = cohen_kappa_score(\n",
    "            y_val,\n",
    "            y_pred, \n",
    "            weights='quadratic'\n",
    "        )\n",
    "\n",
    "        self.val_kappas.append(_val_kappa)\n",
    "\n",
    "        print(f\"val_kappa: {_val_kappa:.4f}\")\n",
    "        \n",
    "        if _val_kappa == max(self.val_kappas):\n",
    "            print(\"Validation Kappa has improved. Saving model.\")\n",
    "            self.model.save('model.h5')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: DenseNet-121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:45.991419Z",
     "start_time": "2019-07-15T20:19:45.982211Z"
    }
   },
   "outputs": [],
   "source": [
    "# xception = Xception(\n",
    "#     include_top=False,\n",
    "#    input_shape=(224, 224, 3)\n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.148301Z",
     "start_time": "2019-07-15T20:19:45.992523Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a7ab210f85a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.149542Z",
     "start_time": "2019-07-15T20:18:17.103Z"
    }
   },
   "outputs": [],
   "source": [
    "size=256\n",
    "densenet = DenseNet121(\n",
    "    include_top=False,\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "img_input = Input(shape=(size, size, 3, ))\n",
    "img_out = densenet(img_input)\n",
    "img_out = GlobalAveragePooling2D()(img_out)\n",
    "img_out = Dropout(0.3)(img_out)\n",
    "img_out = Dense(1024, activation='relu')(img_out)\n",
    "\n",
    "meta_input = Input(shape=(5, ))\n",
    "meta_out = Dense(256, activation=\"relu\")(meta_input)\n",
    "meta_out = Dropout(0.5)(meta_out)\n",
    "meta_out = Dense(256, activation=\"relu\")(meta_out)\n",
    "\n",
    "merged = Concatenate()([img_out, meta_out])\n",
    "output = Dropout(0.3)(merged)\n",
    "output = Dense(5, activation=\"sigmoid\")(output)\n",
    "\n",
    "model = Model(inputs=[img_input, meta_input], output=output)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(lr=0.00005),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.150266Z",
     "start_time": "2019-07-15T20:18:17.104Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.150953Z",
     "start_time": "2019-07-15T20:18:17.106Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot_model(model, to_file=\"metamodel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.151602Z",
     "start_time": "2019-07-15T20:18:17.109Z"
    }
   },
   "outputs": [],
   "source": [
    "kappa_metrics = Metrics()\n",
    "\n",
    "history = model.fit_generator(\n",
    "    data_generator,\n",
    "    steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=([x_val, m_val], y_val),\n",
    "    callbacks=[kappa_metrics]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.152262Z",
     "start_time": "2019-07-15T20:18:17.111Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('xception_history.json', 'w') as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.153225Z",
     "start_time": "2019-07-15T20:18:17.112Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[['loss', 'val_loss']].plot()\n",
    "history_df[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.154257Z",
     "start_time": "2019-07-15T20:18:17.113Z"
    }
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[['loss', 'val_loss']].plot()\n",
    "history_df[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.155221Z",
     "start_time": "2019-07-15T20:18:17.114Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(kappa_metrics.val_kappas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD test set with  bengrahm preprocessing ffs, before final prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.156185Z",
     "start_time": "2019-07-15T20:18:17.116Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.load_weights('model_bengrahm.h5')\n",
    "y_val_pred = model.predict(x_val)\n",
    "\n",
    "def compute_score_inv(threshold):\n",
    "    y1 = y_val_pred > threshold\n",
    "    y1 = y1.astype(int).sum(axis=1) - 1\n",
    "    y2 = y_val.sum(axis=1) - 1\n",
    "    score = cohen_kappa_score(y1, y2, weights='quadratic')\n",
    "    \n",
    "    return 1 - score\n",
    "\n",
    "simplex = scipy.optimize.minimize(\n",
    "    compute_score_inv, 0.5, method='nelder-mead'\n",
    ")\n",
    "\n",
    "best_threshold = simplex['x'][0]\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.157254Z",
     "start_time": "2019-07-15T20:18:17.117Z"
    }
   },
   "outputs": [],
   "source": [
    "#best_threshold = 0.45\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.158355Z",
     "start_time": "2019-07-15T20:18:17.118Z"
    }
   },
   "outputs": [],
   "source": [
    "for th in [0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55]: \n",
    "    y_test = model.predict(x_test) > th\n",
    "    y_test = y_test.astype(int).sum(axis=1) - 1\n",
    "    print(th, np.unique(y_test, return_counts=True))\n",
    "# test_df['diagnosis'] = y_test\n",
    "# test_df.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.159304Z",
     "start_time": "2019-07-15T20:18:17.119Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:46.160224Z",
     "start_time": "2019-07-15T20:18:17.120Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
